{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning how to move a human arm\n",
    "\n",
    "In this tutorial we will show how to train a basic biomechanical model using `keras-rl`.\n",
    "\n",
    "## Installation\n",
    "\n",
    "To make it work, follow the instructions in\n",
    "https://github.com/stanfordnmbl/osim-rl#getting-started\n",
    "i.e. run\n",
    "\n",
    "    conda create -n opensim-rl -c kidzik opensim git python=2.7\n",
    "    source activate opensim-rl\n",
    "    pip install git+https://github.com/stanfordnmbl/osim-rl.git\n",
    "Then run\n",
    "\n",
    "    git clone https://github.com/stanfordnmbl/osim-rl.git\n",
    "    conda install keras -c conda-forge\n",
    "    pip install git+https://github.com/matthiasplappert/keras-rl.git\n",
    "    cd osim-rl\n",
    "    conda install jupyter\n",
    "follow the instructions and once jupyter is installed and type\n",
    "\n",
    "    jupyter notebook\n",
    "This should open the browser with jupyter. Navigate to this notebook, i.e. to the file `scripts/train.arm.ipynb`.\n",
    "\n",
    "## Preparing the environment\n",
    "\n",
    "The following two blocks load necessary libraries and create a simulator environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Derived from keras-rl\n",
    "import opensim as osim\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Input, concatenate\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rl.agents import DDPGAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "\n",
    "from osim.env.arm import ArmEnv\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import argparse\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load walking environment\n",
    "env = ArmEnv(True)\n",
    "env.reset()\n",
    "\n",
    "# Total number of steps in training\n",
    "nallsteps = 10000\n",
    "\n",
    "nb_actions = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the actor and the critic\n",
    "\n",
    "The actor serves as a brain for controlling muscles. The critic is our approximation of how good is the brain performing for achieving the goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 14)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                480       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 198       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 2,790.0\n",
      "Trainable params: 2,790.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create networks for DDPG\n",
    "# Next, we build a very simple model.\n",
    "actor = Sequential()\n",
    "actor.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "actor.add(Dense(32))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(32))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(32))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(nb_actions))\n",
    "actor.add(Activation('sigmoid'))\n",
    "print(actor.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "observation_input (InputLayer)   (None, 1, 14)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "action_input (InputLayer)        (None, 6)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 14)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 64)            1344                                         \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 64)            4160                                         \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 64)            4160                                         \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 1)             65                                           \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 1)             0                                            \n",
      "====================================================================================================\n",
      "Total params: 9,729.0\n",
      "Trainable params: 9,729.0\n",
      "Non-trainable params: 0.0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "action_input = Input(shape=(nb_actions,), name='action_input')\n",
    "observation_input = Input(shape=(1,) + env.observation_space.shape, name='observation_input')\n",
    "flattened_observation = Flatten()(observation_input)\n",
    "x = concatenate([action_input, flattened_observation])\n",
    "x = Dense(64)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(64)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(64)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(1)(x)\n",
    "x = Activation('linear')(x)\n",
    "critic = Model(inputs=[action_input, observation_input], outputs=x)\n",
    "print(critic.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the actor and the critic\n",
    "\n",
    "We will now run `keras-rl` implementation of the DDPG algorithm which trains both networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named mkl\n",
      "[2017-07-22 22:32:57,326] install mkl with `conda install mkl-service`: No module named mkl\n"
     ]
    }
   ],
   "source": [
    "# Set up the agent for training\n",
    "memory = SequentialMemory(limit=100000, window_length=1)\n",
    "random_process = OrnsteinUhlenbeckProcess(theta=.15, mu=0., sigma=.2, size=env.noutput)\n",
    "agent = DDPGAgent(nb_actions=nb_actions, actor=actor, critic=critic, critic_action_input=action_input,\n",
    "                  memory=memory, nb_steps_warmup_critic=100, nb_steps_warmup_actor=100,\n",
    "                  random_process=random_process, gamma=.99, target_model_update=1e-3,\n",
    "                  delta_clip=1.)\n",
    "agent.compile(Adam(lr=.001, clipnorm=1.), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distance: 0.460324\n",
      "True positions: (-1.149648,-0.048636)\n",
      "Reached: (-1.566472,-0.005135)\n",
      "\n",
      "Distance: 0.358700\n",
      "True positions: (0.080656,-0.269618)\n",
      "Reached: (-0.037848,-0.509815)\n",
      "\n",
      "Distance: 2.262523\n",
      "True positions: (0.010504,-0.686305)\n",
      "Reached: (-1.567794,-0.002080)\n",
      "\n",
      "Distance: 0.637941\n",
      "True positions: (-0.651515,-0.027902)\n",
      "Reached: (-0.421993,-0.436321)\n",
      "\n",
      "Distance: 1.889324\n",
      "True positions: (0.043834,-0.279527)\n",
      "Reached: (-1.567862,-0.001899)\n",
      "\n",
      "Distance: 0.467375\n",
      "True positions: (0.068209,-0.205440)\n",
      "Reached: (-0.298954,-0.305652)\n",
      "\n",
      "Distance: 1.345630\n",
      "True positions: (-1.105126,-0.895527)\n",
      "Reached: (-1.564775,-0.009546)\n",
      "\n",
      "Distance: 0.629321\n",
      "True positions: (-0.601926,-0.867793)\n",
      "Reached: (-0.531817,-0.308581)\n",
      "\n",
      "Distance: 1.611037\n",
      "True positions: (-0.589761,-0.644584)\n",
      "Reached: (-1.565031,-0.008817)\n",
      "\n",
      "Distance: 0.639025\n",
      "True positions: (-0.590143,-0.529063)\n",
      "Reached: (-0.167579,-0.312602)\n",
      "\n",
      "Distance: 1.425042\n",
      "True positions: (-0.717768,-0.583929)\n",
      "Reached: (-1.565764,-0.006883)\n",
      "\n",
      "Distance: 0.897903\n",
      "True positions: (-0.282552,-0.845413)\n",
      "Reached: (0.060526,-0.290588)\n",
      "\n",
      "Distance: 2.104613\n",
      "True positions: (-0.258825,-0.797847)\n",
      "Reached: (-1.567617,-0.002026)\n",
      "\n",
      "Distance: 0.754101\n",
      "True positions: (-0.746036,-0.157657)\n",
      "Reached: (-0.102625,-0.268347)\n",
      "\n",
      "Distance: 2.080117\n",
      "True positions: (-0.088709,-0.603127)\n",
      "Reached: (-1.567698,-0.001999)\n",
      "\n",
      "Distance: 0.850010\n",
      "True positions: (-0.159947,-0.403816)\n",
      "Reached: (0.471096,-0.184849)\n",
      "\n",
      "Distance: 0.953120\n",
      "True positions: (-0.774693,-0.161867)\n",
      "Reached: (-1.567858,-0.001912)\n",
      "\n",
      "Distance: 1.623438\n",
      "True positions: (0.000107,-0.715426)\n",
      "Reached: (0.140660,-2.198311)\n",
      "\n",
      "Distance: 1.519730\n",
      "True positions: (-0.156049,-0.109846)\n",
      "Reached: (-1.567854,-0.001921)\n",
      "\n",
      "Distance: 2.551398\n",
      "True positions: (-0.341076,-0.137298)\n",
      "Reached: (-0.127004,-4.083158)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8aadceaf50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "agent.fit(env, nb_steps=2000, visualize=False, verbose=0, nb_max_episode_steps=200, log_interval=10000)\n",
    "# After training is done, we save the final weights.\n",
    "#    agent.save_weights(args.model, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the results\n",
    "Check how our trained 'brain' performs. Below we will also load a pretrained model (on the larger number of episodes), which should perform better. It was trained exactly the same way, just with a larger number of steps (parameter `nb_steps` in `agent.fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 2 episodes ...\n",
      "\n",
      "Distance: 1.744610\n",
      "True positions: (-0.117997,-0.297044)\n",
      "Reached: (-1.567597,-0.002033)\n",
      "\n",
      "Distance: 0.880632\n",
      "True positions: (-0.014510,-0.320320)\n",
      "Reached: (0.339653,-0.846790)\n",
      "\n",
      "Distance: 0.656555\n",
      "True positions: (-0.423018,-0.626791)\n",
      "Reached: (0.056300,-0.804027)\n",
      "\n",
      "Distance: 1.092906\n",
      "True positions: (-0.550707,-0.236474)\n",
      "Reached: (0.175475,-0.603198)\n",
      "\n",
      "Distance: 1.005385\n",
      "True positions: (-0.443725,-0.151066)\n",
      "Reached: (0.203324,-0.509402)\n",
      "\n",
      "Distance: 0.951899\n",
      "True positions: (-0.414841,-0.014396)\n",
      "Reached: (0.123739,-0.427716)\n",
      "\n",
      "Distance: 0.839426\n",
      "True positions: (-0.418568,-0.693720)\n",
      "Reached: (-0.001927,-1.116504)\n",
      "\n",
      "Distance: 1.497009\n",
      "True positions: (-0.894740,-0.584725)\n",
      "Reached: (0.170642,-1.016351)\n",
      "\n",
      "Distance: 1.131753\n",
      "True positions: (-0.743247,-0.725087)\n",
      "Reached: (0.183012,-0.930581)\n",
      "\n",
      "Distance: 0.312900\n",
      "True positions: (-0.142488,-0.953783)\n",
      "Reached: (0.149220,-0.932591)\n",
      "Episode 1: reward: -976.410, steps: 1000\n",
      "\n",
      "Distance: 1.580191\n",
      "True positions: (-0.732438,-0.747050)\n",
      "Reached: (-1.567608,-0.002028)\n",
      "\n",
      "Distance: 0.556819\n",
      "True positions: (0.165693,-0.517732)\n",
      "Reached: (0.298295,-0.941949)\n",
      "\n",
      "Distance: 0.852716\n",
      "True positions: (-0.431979,-0.202319)\n",
      "Reached: (-0.004357,-0.627413)\n",
      "\n",
      "Distance: 0.061322\n",
      "True positions: (0.165523,-0.588966)\n",
      "Reached: (0.199187,-0.561308)\n",
      "\n",
      "Distance: 0.222607\n",
      "True positions: (0.181837,-0.303762)\n",
      "Reached: (0.124651,-0.469183)\n",
      "\n",
      "Distance: 0.759515\n",
      "True positions: (-0.399265,-0.265959)\n",
      "Reached: (0.098984,-0.527225)\n",
      "\n",
      "Distance: 0.217864\n",
      "True positions: (-0.048969,-0.635982)\n",
      "Reached: (0.153119,-0.651758)\n",
      "\n",
      "Distance: 1.146655\n",
      "True positions: (-0.664945,-0.014114)\n",
      "Reached: (0.026750,-0.469074)\n",
      "\n",
      "Distance: 0.396665\n",
      "True positions: (0.188923,-0.061788)\n",
      "Reached: (0.170766,-0.440296)\n",
      "\n",
      "Distance: 1.534696\n",
      "True positions: (-0.972415,-0.597262)\n",
      "Reached: (0.166697,-0.992847)\n",
      "Episode 2: reward: -857.089, steps: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8aa0777410>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agent.load_weights(args.model)\n",
    "# Finally, evaluate our algorithm for 1 episode.\n",
    "agent.test(env, nb_episodes=2, visualize=False, nb_max_episode_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "\n",
      "Distance: 1.982073\n",
      "True positions: (-0.365306,-0.789004)\n",
      "Reached: (-1.565560,-0.007185)\n",
      "\n",
      "Distance: 0.489316\n",
      "True positions: (-1.066750,-0.081859)\n",
      "Reached: (-0.828360,-0.332785)\n",
      "\n",
      "Distance: 0.340835\n",
      "True positions: (-0.821318,-0.029109)\n",
      "Reached: (-0.726064,-0.274690)\n",
      "\n",
      "Distance: 0.112628\n",
      "True positions: (-0.885140,-0.309075)\n",
      "Reached: (-0.880001,-0.416565)\n",
      "\n",
      "Distance: 0.225917\n",
      "True positions: (-1.052513,-0.912940)\n",
      "Reached: (-0.975606,-1.061949)\n",
      "\n",
      "Distance: 0.219576\n",
      "True positions: (-0.137413,-0.078975)\n",
      "Reached: (-0.165256,-0.270706)\n",
      "\n",
      "Distance: 0.201277\n",
      "True positions: (-0.692787,-0.661610)\n",
      "Reached: (-0.850584,-0.705090)\n",
      "\n",
      "Distance: 0.257323\n",
      "True positions: (-0.855284,-0.132450)\n",
      "Reached: (-0.740171,-0.274659)\n",
      "\n",
      "Distance: 0.099240\n",
      "True positions: (-0.470129,-0.782663)\n",
      "Reached: (-0.568455,-0.781748)\n",
      "\n",
      "Distance: 0.418505\n",
      "True positions: (-1.123986,-0.081918)\n",
      "Reached: (-0.917664,-0.294101)\n",
      "Episode 1: reward: -340.685, steps: 1000\n",
      "\n",
      "Distance: 0.565987\n",
      "True positions: (-1.166045,-0.176900)\n",
      "Reached: (-1.564738,-0.009606)\n",
      "\n",
      "Distance: 0.413008\n",
      "True positions: (-1.111553,-0.424746)\n",
      "Reached: (-0.708131,-0.434332)\n",
      "\n",
      "Distance: 0.160070\n",
      "True positions: (0.187228,-0.845948)\n",
      "Reached: (0.335622,-0.857624)\n",
      "\n",
      "Distance: 0.014350\n",
      "True positions: (-0.234847,-0.743126)\n",
      "Reached: (-0.222865,-0.740759)\n",
      "\n",
      "Distance: 0.218572\n",
      "True positions: (0.017750,-0.083022)\n",
      "Reached: (0.066215,-0.253128)\n",
      "\n",
      "Distance: 0.137725\n",
      "True positions: (-0.973785,-0.580600)\n",
      "Reached: (-1.055013,-0.637097)\n",
      "\n",
      "Distance: 0.063488\n",
      "True positions: (-0.050015,-0.270118)\n",
      "Reached: (-0.033164,-0.316754)\n",
      "\n",
      "Distance: 0.145679\n",
      "True positions: (-0.577268,-0.582374)\n",
      "Reached: (-0.706824,-0.598496)\n",
      "\n",
      "Distance: 0.040211\n",
      "True positions: (-0.849001,-0.538548)\n",
      "Reached: (-0.888245,-0.537582)\n",
      "\n",
      "Distance: 0.242493\n",
      "True positions: (0.150636,-0.690794)\n",
      "Reached: (0.286809,-0.584474)\n",
      "Episode 2: reward: -283.193, steps: 1000\n",
      "\n",
      "Distance: 1.110902\n",
      "True positions: (-0.911919,-0.467695)\n",
      "Reached: (-1.564734,-0.009608)\n",
      "\n",
      "Distance: 0.187479\n",
      "True positions: (-0.912437,-0.926943)\n",
      "Reached: (-0.870454,-1.072439)\n",
      "\n",
      "Distance: 0.692823\n",
      "True positions: (-1.166263,-0.060925)\n",
      "Reached: (-0.728677,-0.316163)\n",
      "\n",
      "Distance: 0.121699\n",
      "True positions: (0.069839,-0.985841)\n",
      "Reached: (0.183458,-0.977761)\n",
      "\n",
      "Distance: 0.362258\n",
      "True positions: (-0.728851,-0.079597)\n",
      "Reached: (-0.620598,-0.333603)\n",
      "\n",
      "Distance: 0.305197\n",
      "True positions: (-0.359902,-0.059023)\n",
      "Reached: (-0.353848,-0.358166)\n",
      "\n",
      "Distance: 0.177422\n",
      "True positions: (-0.852094,-0.251452)\n",
      "Reached: (-0.762637,-0.339417)\n",
      "\n",
      "Distance: 0.168020\n",
      "True positions: (0.197237,-0.513982)\n",
      "Reached: (0.287570,-0.436296)\n",
      "\n",
      "Distance: 0.113966\n",
      "True positions: (-0.398864,-0.999874)\n",
      "Reached: (-0.479416,-0.966461)\n",
      "\n",
      "Distance: 0.052240\n",
      "True positions: (-0.201805,-0.521741)\n",
      "Reached: (-0.168578,-0.502727)\n",
      "Episode 3: reward: -306.522, steps: 1000\n",
      "\n",
      "Distance: 1.525887\n",
      "True positions: (-0.395860,-0.363372)\n",
      "Reached: (-1.565560,-0.007185)\n",
      "\n",
      "Distance: 0.075686\n",
      "True positions: (-0.862836,-0.487829)\n",
      "Reached: (-0.796294,-0.478685)\n",
      "\n",
      "Distance: 0.254760\n",
      "True positions: (-0.630002,-0.431469)\n",
      "Reached: (-0.768987,-0.547243)\n",
      "\n",
      "Distance: 0.149572\n",
      "True positions: (-0.852366,-0.310090)\n",
      "Reached: (-0.896241,-0.415787)\n",
      "\n",
      "Distance: 0.255215\n",
      "True positions: (-0.690679,-0.037590)\n",
      "Reached: (-0.661083,-0.263208)\n",
      "\n",
      "Distance: 0.078589\n",
      "True positions: (0.115092,-0.366882)\n",
      "Reached: (0.160598,-0.333799)\n",
      "\n",
      "Distance: 0.041458\n",
      "True positions: (-0.237613,-0.866449)\n",
      "Reached: (-0.246931,-0.834309)\n",
      "\n",
      "Distance: 0.078235\n",
      "True positions: (-0.091389,-0.404260)\n",
      "Reached: (-0.055849,-0.446955)\n",
      "\n",
      "Distance: 0.110965\n",
      "True positions: (-0.014442,-0.979859)\n",
      "Reached: (0.075633,-0.958968)\n",
      "\n",
      "Distance: 0.159591\n",
      "True positions: (0.223128,-0.570212)\n",
      "Reached: (0.298659,-0.486151)\n",
      "Episode 4: reward: -238.329, steps: 1000\n",
      "\n",
      "Distance: 1.908310\n",
      "True positions: (-0.300740,-0.643097)\n",
      "Reached: (-1.567860,-0.001908)\n",
      "\n",
      "Distance: 0.173744\n",
      "True positions: (0.154661,-0.377354)\n",
      "Reached: (0.255312,-0.450447)\n",
      "\n",
      "Distance: 0.139088\n",
      "True positions: (-0.818098,-0.352755)\n",
      "Reached: (-0.855547,-0.454393)\n",
      "\n",
      "Distance: 0.193557\n",
      "True positions: (-0.676368,-0.192755)\n",
      "Reached: (-0.587963,-0.297906)\n",
      "\n",
      "Distance: 0.157934\n",
      "True positions: (-0.810920,-0.369649)\n",
      "Reached: (-0.867354,-0.471150)\n",
      "\n",
      "Distance: 0.032535\n",
      "True positions: (-0.838878,-0.567574)\n",
      "Reached: (-0.836916,-0.537001)\n",
      "\n",
      "Distance: 0.051300\n",
      "True positions: (-0.155710,-0.606622)\n",
      "Reached: (-0.115568,-0.595464)\n",
      "\n",
      "Distance: 0.009658\n",
      "True positions: (-0.148966,-0.381940)\n",
      "Reached: (-0.143162,-0.385795)\n",
      "\n",
      "Distance: 0.117823\n",
      "True positions: (-0.476770,-0.817293)\n",
      "Reached: (-0.591291,-0.820595)\n",
      "\n",
      "Distance: 0.161973\n",
      "True positions: (-0.971441,-0.233340)\n",
      "Reached: (-0.881267,-0.305139)\n",
      "Episode 5: reward: -198.009, steps: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8aa0777890>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.load_weights(\"../models/example.h5f\")\n",
    "# Finally, evaluate our algorithm for 1 episode.\n",
    "agent.test(env, nb_episodes=5, visualize=False, nb_max_episode_steps=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
